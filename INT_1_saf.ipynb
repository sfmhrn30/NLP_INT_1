{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wIcxFhYjGIQ",
        "outputId": "e0b373f4-2f0c-4185-e619-746ed0dbb27b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement ntlk (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for ntlk\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install ntlk\n",
        "from nltk import download\n",
        "download(\"stopwords\")\n",
        "download(\"punkt\")\n",
        "download(\"wordnet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2_exp"
      ],
      "metadata": {
        "id": "A2l8B9vZmpib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sample for nltk code\n",
        "# stop word removal\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize as WT\n",
        "\n",
        "print(stopwords.words('english'))\n",
        "StopWords=stopwords.words('english')\n",
        "\n",
        "sent = \"The quick brown fox jumped. Over a lazy dog! #Dog#Fox. Dancing\"\n",
        "wt_tokens = WT(sent)\n",
        "\n",
        "after_removing_stopwords = [ w for w in wt_tokens if w.lower() not in StopWords]\n",
        "print(after_removing_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLTioNNdrJ2G",
        "outputId": "4eed5fba-a555-464f-dac4-15d1c37e66f0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "['quick', 'brown', 'fox', 'jumped', '.', 'lazy', 'dog', '!', '#', 'Dog', '#', 'Fox', '.', 'Dancing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: small sample code for spacy\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Create a spaCy document\n",
        "doc = nlp(\"The quick brown fox jumps over the lazy dog\")\n",
        "\n",
        "# Print the tokens and their part-of-speech tags\n",
        "for token in doc:\n",
        "  print(token.text, token.pos_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmYLlh2gmsYK",
        "outputId": "4ba597f8-8947-48c0-c97e-6da62e0ecf13"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The DET\n",
            "quick ADJ\n",
            "brown ADJ\n",
            "fox NOUN\n",
            "jumps VERB\n",
            "over ADP\n",
            "the DET\n",
            "lazy ADJ\n",
            "dog NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: small sample code for Gensim\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Create a list of sentences\n",
        "sentences = [[\"the\", \"quick\", \"brown\", \"fox\", \"jumps\"], [\"over\", \"the\", \"lazy\", \"dog\"]]\n",
        "\n",
        "# Train the Word2Vec model\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "\n",
        "# Print the vocabulary\n",
        "print(model.wv.index_to_key)\n",
        "\n",
        "# Print the vector for the word \"fox\"\n",
        "print(model.wv[\"fox\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRvUM9_bmsmW",
        "outputId": "88b0ced4-77d0-4785-b68c-c7f6612f9a74"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'dog', 'lazy', 'over', 'jumps', 'fox', 'brown', 'quick']\n",
            "[-8.7274825e-03  2.1301615e-03 -8.7354420e-04 -9.3190884e-03\n",
            " -9.4281426e-03 -1.4107180e-03  4.4324086e-03  3.7040710e-03\n",
            " -6.4986930e-03 -6.8730675e-03 -4.9994122e-03 -2.2868442e-03\n",
            " -7.2502876e-03 -9.6033178e-03 -2.7436293e-03 -8.3628409e-03\n",
            " -6.0388758e-03 -5.6709289e-03 -2.3441375e-03 -1.7069972e-03\n",
            " -8.9569986e-03 -7.3519943e-04  8.1525063e-03  7.6904297e-03\n",
            " -7.2061159e-03 -3.6668312e-03  3.1185520e-03 -9.5707225e-03\n",
            "  1.4764392e-03  6.5244664e-03  5.7464195e-03 -8.7630618e-03\n",
            " -4.5171441e-03 -8.1401607e-03  4.5956374e-05  9.2636338e-03\n",
            "  5.9733056e-03  5.0673080e-03  5.0610625e-03 -3.2429171e-03\n",
            "  9.5521836e-03 -7.3564244e-03 -7.2703874e-03 -2.2653891e-03\n",
            " -7.7856064e-04 -3.2161034e-03 -5.9258583e-04  7.4888230e-03\n",
            " -6.9751858e-04 -1.6249407e-03  2.7443992e-03 -8.3591007e-03\n",
            "  7.8558037e-03  8.5361041e-03 -9.5840869e-03  2.4462664e-03\n",
            "  9.9049713e-03 -7.6658037e-03 -6.9669187e-03 -7.7365171e-03\n",
            "  8.3959233e-03 -6.8133592e-04  9.1444086e-03 -8.1582209e-03\n",
            "  3.7430846e-03  2.6350426e-03  7.4271322e-04  2.3276759e-03\n",
            " -7.4690939e-03 -9.3583735e-03  2.3545765e-03  6.1484552e-03\n",
            "  7.9856887e-03  5.7358947e-03 -7.7733636e-04  8.3061643e-03\n",
            " -9.3363142e-03  3.4061326e-03  2.6675343e-04  3.8572443e-03\n",
            "  7.3857834e-03 -6.7251669e-03  5.5844807e-03 -9.5222248e-03\n",
            " -8.0445886e-04 -8.6887367e-03 -5.0986730e-03  9.2892265e-03\n",
            " -1.8582619e-03  2.9144264e-03  9.0712793e-03  8.9381328e-03\n",
            " -8.2084350e-03 -3.0123137e-03  9.8866057e-03  5.1044310e-03\n",
            " -1.5880871e-03 -8.6920215e-03  2.9615164e-03 -6.6758976e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sample for TextBob\n",
        "!python -m textblob.download_corpora\n",
        "from textblob import TextBlob\n",
        "text='I love this lazy dog'\n",
        "blob=TextBlob(text)\n",
        "print(blob.sentiment)\n",
        "nouns=blob.noun_phrases\n",
        "print(nouns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULELrboro7la",
        "outputId": "44261c59-83b7-4e57-c2de-dab8efcc44fb"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "Finished.\n",
            "Sentiment(polarity=0.125, subjectivity=0.8)\n",
            "['lazy dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3_exp"
      ],
      "metadata": {
        "id": "L3mRONjMkuJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization\n",
        "\n",
        "from nltk.tokenize import word_tokenize as WT,sent_tokenize as ST,wordpunct_tokenize as WPT,TweetTokenizer as TT\n",
        "\n",
        "sent = \"The quick brown fox jumped. Over the lazy dog! #Dog#Fox\"\n",
        "wt_tokens = WT(sent)\n",
        "st_tokens = ST(sent)\n",
        "wpt_tokens = WPT(sent)\n",
        "tt_tokens = TT().tokenize(sent)\n",
        "print(wt_tokens)\n",
        "print(st_tokens)\n",
        "print(wpt_tokens)\n",
        "print(tt_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6qWIWYbkw-d",
        "outputId": "699feed5-d02d-44b2-9855-b8050cb0edfc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumped', '.', 'Over', 'the', 'lazy', 'dog', '!', '#', 'Dog', '#', 'Fox']\n",
            "['The quick brown fox jumped.', 'Over the lazy dog!', '#Dog#Fox']\n",
            "['The', 'quick', 'brown', 'fox', 'jumped', '.', 'Over', 'the', 'lazy', 'dog', '!', '#', 'Dog', '#', 'Fox']\n",
            "['The', 'quick', 'brown', 'fox', 'jumped', '.', 'Over', 'the', 'lazy', 'dog', '!', '#Dog', '#Fox']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stop word removal\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize as WT\n",
        "\n",
        "print(stopwords.words('english'))\n",
        "StopWords=stopwords.words('english')\n",
        "\n",
        "sent = \"The quick brown fox jumped. Over a lazy dog! #Dog#Fox. Dancing\"\n",
        "wt_tokens = WT(sent)\n",
        "\n",
        "after_removing_stopwords = [ w for w in wt_tokens if w.lower() not in StopWords]\n",
        "print(after_removing_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCrqLvlNk2if",
        "outputId": "f5b04100-3265-45ca-9b08-a5338af5c76f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "['quick', 'brown', 'fox', 'jumped', '.', 'lazy', 'dog', '!', '#', 'Dog', '#', 'Fox', '.', 'Dancing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "PS = PorterStemmer()\n",
        "\n",
        "stemmed_tokens = [PS.stem(w) for w in after_removing_stopwords]\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e54DLPJClYZ0",
        "outputId": "c2d2cc73-9454-4316-aacf-7631930b52ec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['quick', 'brown', 'fox', 'jump', '.', 'lazi', 'dog', '!', '#', 'dog', '#', 'fox', '.', 'danc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmetization\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "LM = WordNetLemmatizer()\n",
        "lemm_words = [LM.lemmatize(w) for w in after_removing_stopwords]\n",
        "print(lemm_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUHgO0emlhQi",
        "outputId": "89bdc2a4-bc07-40ff-a342-2a1a74b71e1d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['quick', 'brown', 'fox', 'jumped', '.', 'lazy', 'dog', '!', '#', 'Dog', '#', 'Fox', '.', 'Dancing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4_exp\n"
      ],
      "metadata": {
        "id": "W8lRQljVloLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS tagging\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sent = \"The quick brown fox jumped over the lazy dog.\"\n",
        "doc = nlp(sent)\n",
        "\n",
        "for token in doc:\n",
        "  print(f\"{token.text} => {token.pos_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igVP1asTlqh2",
        "outputId": "49716a96-421a-4dbc-b8db-a57aa6444ac2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The => DET\n",
            "quick => ADJ\n",
            "brown => ADJ\n",
            "fox => NOUN\n",
            "jumped => VERB\n",
            "over => ADP\n",
            "the => DET\n",
            "lazy => ADJ\n",
            "dog => NOUN\n",
            ". => PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dependency parsing\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sent = \"The quick brown fox jumped over the lazy dog.\"\n",
        "doc = nlp(sent)\n",
        "\n",
        "for token in doc :\n",
        "  print(f\"{token.text} {token.dep_} > {token.head.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28VAjN36lswc",
        "outputId": "79afc011-e1cf-4ea5-dea6-5432d2443ba1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The det > fox\n",
            "quick amod > fox\n",
            "brown amod > fox\n",
            "fox nsubj > jumped\n",
            "jumped ROOT > jumped\n",
            "over prep > jumped\n",
            "the det > dog\n",
            "lazy amod > dog\n",
            "dog pobj > over\n",
            ". punct > jumped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# co reference resolution\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def printMentions(doc):\n",
        "  for cluster in doc._coref_clusters:\n",
        "    print(cluster.mention)\n",
        "\n",
        "sent = \"the quick brown fox jumped ver the lazy dog\"\n",
        "doc=nlp(sent)\n",
        "printMentions(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "aEwojfQ2lxI1",
        "outputId": "ba782823-38ce-4c65-94ca-dd7f5958c1b0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'spacy.tokens.doc.Doc' object has no attribute '_coref_clusters'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c199d0f4b11e>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"the quick brown fox jumped ver the lazy dog\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprintMentions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-c199d0f4b11e>\u001b[0m in \u001b[0;36mprintMentions\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprintMentions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coref_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'spacy.tokens.doc.Doc' object has no attribute '_coref_clusters'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# named entity relationships\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sent = \"the quick brown fox jumped ver the lazy dog\"\n",
        "doc=nlp(sent)\n",
        "\n",
        "for token in doc.ents:\n",
        "  print(f\"{token.text} => {token.label_}\")\n"
      ],
      "metadata": {
        "id": "EBmvBS0gl9BA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5_exp"
      ],
      "metadata": {
        "id": "zOnGvJDjmNVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encoding\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "corpus = [\n",
        "    'dog bites man',\n",
        "    'man bites dog',\n",
        "    'dog eats meat',\n",
        "    'man eats food'\n",
        "]\n",
        "corpus_reshaped = [[doc] for doc in corpus]\n",
        "\n",
        "ohe = OneHotEncoder(sparse = False)\n",
        "encoded = ohe.fit_transform(corpus_reshaped)\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVqfrD9ZmPhE",
        "outputId": "f12cd60d-2763-47dc-b1fb-4f436bc7284a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 0. 1.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bag of words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "    'dog bites man',\n",
        "    'man bites dog',\n",
        "    'dog eats meat',\n",
        "    'man eats food'\n",
        "]\n",
        "\n",
        "vec = CountVectorizer()\n",
        "x=vec.fit_transform(corpus)\n",
        "\n",
        "print(x.toarray())\n",
        "print(vec.vocabulary_) # shows the indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1o-sS5vomSvh",
        "outputId": "1b48275a-8185-420e-b466-2585af3a2c0a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1 0 0 1 0]\n",
            " [1 1 0 0 1 0]\n",
            " [0 1 1 0 0 1]\n",
            " [0 0 1 1 1 0]]\n",
            "{'dog': 1, 'bites': 0, 'man': 4, 'eats': 2, 'meat': 5, 'food': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bag of N grams\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "    'dog bites man',\n",
        "    'man bites dog',\n",
        "    'dog eats meat',\n",
        "    'man eats food'\n",
        "]\n",
        "\n",
        "vec = CountVectorizer(ngram_range=(1,2))\n",
        "x=vec.fit_transform(corpus)\n",
        "\n",
        "print(x.toarray())\n",
        "print(vec.vocabulary_) # shows the indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejx9mWw2mWLq",
        "outputId": "6b6b8800-1ff4-41ef-af8c-974c36a487d2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0 1 1 1 0 0 0 0 0 1 0 0 0]\n",
            " [1 1 0 1 0 0 0 0 0 0 1 1 0 0]\n",
            " [0 0 0 1 0 1 1 0 1 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 1 1 0 1 1 0 1 0]]\n",
            "{'dog': 3, 'bites': 0, 'man': 10, 'dog bites': 4, 'bites man': 2, 'man bites': 11, 'bites dog': 1, 'eats': 6, 'meat': 13, 'dog eats': 5, 'eats meat': 8, 'food': 9, 'man eats': 12, 'eats food': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = [\n",
        "    'dog bites man',\n",
        "    'man bites dog',\n",
        "    'dog eats meat',\n",
        "    'man eats food'\n",
        "]\n",
        "tfidf = TfidfVectorizer()\n",
        "x = tfidf.fit_transform(corpus)\n",
        "\n",
        "print(x.toarray())\n",
        "print(tfidf.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBCpOsMbmYkA",
        "outputId": "7e01cb35-f441-4f1e-b8d4-96dd8be3781d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
            " [0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
            " [0.         0.44809973 0.55349232 0.         0.         0.70203482]\n",
            " [0.         0.         0.55349232 0.70203482 0.44809973 0.        ]]\n",
            "{'dog': 1, 'bites': 0, 'man': 4, 'eats': 2, 'meat': 5, 'food': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word embeddings\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize as wt\n",
        "corpus = \"A quick brown fox jumps over a lazy dog\"\n",
        "wt_tokens = wt(corpus)\n",
        "x = Word2Vec(wt_tokens, vector_size = 3, window = 5, min_count = 1, workers = 4)\n",
        "vocab = x.wv.index_to_key\n",
        "for token in vocab:\n",
        "  print(token, x.wv[token])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDxIAfM2mcDi",
        "outputId": "e0f54ba9-cda8-4d0f-c59f-120a24bdbdcc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "o [-0.01788738  0.00786891  0.17014939]\n",
            "u [ 0.3004414  -0.31031403 -0.23716971]\n",
            "a [ 0.21529575  0.2990996  -0.16718094]\n",
            "r [-0.12544572  0.24601682 -0.05111571]\n",
            "g [-0.15119685  0.21841606 -0.1619917 ]\n",
            "f [-0.06053392  0.09588599  0.03306246]\n",
            "q [-0.27617383 -0.3149606   0.24372554]\n",
            "i [0.16900873 0.22525644 0.02542885]\n",
            "c [ 0.21169634 -0.1135122  -0.03154671]\n",
            "k [ 0.19228578 -0.25072125 -0.13120346]\n",
            "b [-0.25035182 -0.0310077   0.317952  ]\n",
            "w [-0.24402812 -0.07777423 -0.06461751]\n",
            "n [ 0.2692479  -0.19769652  0.00150541]\n",
            "x [-0.1584578  -0.32011834  0.16690977]\n",
            "d [-0.2919862  -0.14639418 -0.00117   ]\n",
            "j [-0.00982827 -0.25540805  0.32057253]\n",
            "m [ 0.1660686   0.30777144 -0.27193058]\n",
            "p [ 0.14998336 -0.13809635  0.02754064]\n",
            "s [ 0.28339052 -0.14889763  0.15059373]\n",
            "v [-0.226232   -0.11828295  0.3132836 ]\n",
            "e [-0.05258842  0.01071239 -0.13802099]\n",
            "l [-0.2560896  -0.05026694  0.08232649]\n",
            "z [-0.0296009   0.1844554  -0.09143257]\n",
            "y [0.0753355  0.1818598  0.27819845]\n",
            "A [-0.04845802 -0.30693808  0.14568508]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g-5Z5KVGmjBz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}